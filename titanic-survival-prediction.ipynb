{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":3136,"databundleVersionId":26502,"sourceType":"competition"}],"dockerImageVersionId":30822,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 1: Import libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Load the datasets\ntrain_df = pd.read_csv('/kaggle/input/titanic/train.csv')\ntest_df = pd.read_csv('/kaggle/input/titanic/test.csv')\ngender_submission_df = pd.read_csv('/kaggle/input/titanic/gender_submission.csv')\n\n# Display the first few rows of the datasets\nprint(\"Train Dataset Overview:\")\ndisplay(train_df.head())\nprint(\"Test Dataset Overview:\")\ndisplay(test_df.head())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-28T11:22:25.167487Z","iopub.execute_input":"2024-12-28T11:22:25.167853Z","iopub.status.idle":"2024-12-28T11:22:27.493662Z","shell.execute_reply.started":"2024-12-28T11:22:25.167809Z","shell.execute_reply":"2024-12-28T11:22:27.492775Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create a summary of missing values for train and test datasets\nmissing_summary = pd.DataFrame({\n    'Missing Values (Train)': missing_train,\n    'Missing Values (Test)': missing_test\n}).sort_values(by='Missing Values (Train)', ascending=False)\n\n# Reset the index for better readability\nmissing_summary.reset_index(inplace=True)\nmissing_summary.rename(columns={'index': 'Feature'}, inplace=True)\n\n# Display the missing values summary\nprint(\"Missing Values Summary:\")\ndisplay(missing_summary)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-28T11:31:57.886545Z","iopub.execute_input":"2024-12-28T11:31:57.886921Z","iopub.status.idle":"2024-12-28T11:31:57.915383Z","shell.execute_reply.started":"2024-12-28T11:31:57.886892Z","shell.execute_reply":"2024-12-28T11:31:57.914232Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 1: Create a new binary column `HasCabin`\ntrain_df['HasCabin'] = train_df['Cabin'].notnull().astype(int)\ntest_df['HasCabin'] = test_df['Cabin'].notnull().astype(int)\n\n# Step 2: Drop the original `Cabin` column\ntrain_df.drop(columns=['Cabin'], inplace=True)\ntest_df.drop(columns=['Cabin'], inplace=True)\n\n# Verify changes\nprint(\"Updated Train Dataset:\")\ndisplay(train_df.head())\n\nprint(\"Updated Test Dataset:\")\ndisplay(test_df.head())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-28T11:32:54.342674Z","iopub.execute_input":"2024-12-28T11:32:54.343050Z","iopub.status.idle":"2024-12-28T11:32:54.378952Z","shell.execute_reply.started":"2024-12-28T11:32:54.343020Z","shell.execute_reply":"2024-12-28T11:32:54.377620Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 1: Define a function to fill missing age values based on Pclass and Sex\ndef fill_missing_age(df):\n    df['Age'] = df.groupby(['Pclass', 'Sex'])['Age'].transform(lambda x: x.fillna(x.median()))\n    return df\n\n# Step 2: Apply the function to both train and test datasets\ntrain_df = fill_missing_age(train_df)\ntest_df = fill_missing_age(test_df)\n\n# Step 3: Verify changes\nprint(\"Missing values in 'Age' (Train):\", train_df['Age'].isnull().sum())\nprint(\"Missing values in 'Age' (Test):\", test_df['Age'].isnull().sum())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-28T11:33:55.260944Z","iopub.execute_input":"2024-12-28T11:33:55.261274Z","iopub.status.idle":"2024-12-28T11:33:55.283431Z","shell.execute_reply.started":"2024-12-28T11:33:55.261248Z","shell.execute_reply":"2024-12-28T11:33:55.282557Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 1: Fill missing values in 'Embarked' with the mode\nembarked_mode = train_df['Embarked'].mode()[0]\ntrain_df['Embarked'].fillna(embarked_mode, inplace=True)\n\n# Step 2: Verify the changes\nprint(\"Missing values in 'Embarked' (Train):\", train_df['Embarked'].isnull().sum())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-28T11:34:27.020875Z","iopub.execute_input":"2024-12-28T11:34:27.021236Z","iopub.status.idle":"2024-12-28T11:34:27.031428Z","shell.execute_reply.started":"2024-12-28T11:34:27.021206Z","shell.execute_reply":"2024-12-28T11:34:27.030426Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 1: Fill missing value in 'Fare' with the median\nfare_median = test_df['Fare'].median()\ntest_df['Fare'].fillna(fare_median, inplace=True)\n\n# Step 2: Verify the changes\nprint(\"Missing values in 'Fare' (Test):\", test_df['Fare'].isnull().sum())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-28T11:34:46.457996Z","iopub.execute_input":"2024-12-28T11:34:46.458381Z","iopub.status.idle":"2024-12-28T11:34:46.465822Z","shell.execute_reply.started":"2024-12-28T11:34:46.458349Z","shell.execute_reply":"2024-12-28T11:34:46.464906Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 1: Encode 'Sex' as binary\ntrain_df['Sex'] = train_df['Sex'].map({'male': 0, 'female': 1})\ntest_df['Sex'] = test_df['Sex'].map({'male': 0, 'female': 1})\n\n# Step 2: One-hot encode 'Embarked'\ntrain_df = pd.get_dummies(train_df, columns=['Embarked'], prefix='Embarked')\ntest_df = pd.get_dummies(test_df, columns=['Embarked'], prefix='Embarked')\n\n# Ensure both datasets have the same columns\nmissing_cols = set(train_df.columns) - set(test_df.columns)\nfor col in missing_cols:\n    test_df[col] = 0\ntest_df = test_df[train_df.columns.drop('Survived')]\n\n# Verify changes\nprint(\"Encoded Train Dataset:\")\ndisplay(train_df.head())\n\nprint(\"Encoded Test Dataset:\")\ndisplay(test_df.head())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-28T11:36:59.322537Z","iopub.execute_input":"2024-12-28T11:36:59.322934Z","iopub.status.idle":"2024-12-28T11:36:59.362054Z","shell.execute_reply.started":"2024-12-28T11:36:59.322900Z","shell.execute_reply":"2024-12-28T11:36:59.361244Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 1: Extract titles from 'Name'\ntrain_df['Title'] = train_df['Name'].str.extract(r',\\s*([^\\.]+)\\.', expand=False)\ntest_df['Title'] = test_df['Name'].str.extract(r',\\s*([^\\.]+)\\.', expand=False)\n\n# Step 2: Group rare titles together\ntitle_mapping = {\n    \"Mlle\": \"Miss\", \"Ms\": \"Miss\", \"Mme\": \"Mrs\", \n    \"Lady\": \"Royalty\", \"Countess\": \"Royalty\", \"Sir\": \"Royalty\", \"Jonkheer\": \"Royalty\", \"Don\": \"Royalty\", \"Dona\": \"Royalty\", \n    \"Capt\": \"Officer\", \"Col\": \"Officer\", \"Major\": \"Officer\", \"Dr\": \"Officer\", \"Rev\": \"Officer\"\n}\ntrain_df['Title'] = train_df['Title'].replace(title_mapping)\ntest_df['Title'] = test_df['Title'].replace(title_mapping)\n\n# Step 3: Encode titles as numeric\ntitle_order = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Royalty\": 5, \"Officer\": 6}\ntrain_df['Title'] = train_df['Title'].map(title_order)\ntest_df['Title'] = test_df['Title'].map(title_order)\n\n# Step 4: Drop 'Name' column\ntrain_df.drop(columns=['Name'], inplace=True)\ntest_df.drop(columns=['Name'], inplace=True)\n\n# Verify changes\nprint(\"Updated Train Dataset:\")\ndisplay(train_df.head())\n\nprint(\"Updated Test Dataset:\")\ndisplay(test_df.head())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-28T11:37:59.217753Z","iopub.execute_input":"2024-12-28T11:37:59.218099Z","iopub.status.idle":"2024-12-28T11:37:59.263231Z","shell.execute_reply.started":"2024-12-28T11:37:59.218073Z","shell.execute_reply":"2024-12-28T11:37:59.262391Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 1: Create 'FamilySize' feature\ntrain_df['FamilySize'] = train_df['SibSp'] + train_df['Parch'] + 1\ntest_df['FamilySize'] = test_df['SibSp'] + test_df['Parch'] + 1\n\n# Step 2: Create 'IsAlone' feature (1 if FamilySize == 1, else 0)\ntrain_df['IsAlone'] = (train_df['FamilySize'] == 1).astype(int)\ntest_df['IsAlone'] = (test_df['FamilySize'] == 1).astype(int)\n\n# Step 3: Verify changes\nprint(\"Updated Train Dataset with FamilySize and IsAlone:\")\ndisplay(train_df.head())\n\nprint(\"Updated Test Dataset with FamilySize and IsAlone:\")\ndisplay(test_df.head())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-28T11:52:54.602214Z","iopub.execute_input":"2024-12-28T11:52:54.602591Z","iopub.status.idle":"2024-12-28T11:52:54.633197Z","shell.execute_reply.started":"2024-12-28T11:52:54.602564Z","shell.execute_reply":"2024-12-28T11:52:54.632383Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Check for missing values in X_train and X_val\nprint(\"Missing Values in Training Features:\")\nprint(X_train.isnull().sum()[X_train.isnull().sum() > 0])\n\nprint(\"\\nMissing Values in Validation Features:\")\nprint(X_val.isnull().sum()[X_val.isnull().sum() > 0])\n\n# Fill missing values with median (or mean as appropriate)\nX_train.fillna(X_train.median(), inplace=True)\nX_val.fillna(X_val.median(), inplace=True)\n\n# Recheck for missing values after filling\nprint(\"\\nMissing Values in Training Features (Post-Fill):\")\nprint(X_train.isnull().sum().sum())\n\nprint(\"\\nMissing Values in Validation Features (Post-Fill):\")\nprint(X_val.isnull().sum().sum())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-28T11:55:06.120807Z","iopub.execute_input":"2024-12-28T11:55:06.121151Z","iopub.status.idle":"2024-12-28T11:55:06.151077Z","shell.execute_reply.started":"2024-12-28T11:55:06.121125Z","shell.execute_reply":"2024-12-28T11:55:06.149855Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 1: Train Logistic Regression model\nmodel = LogisticRegression(max_iter=1000, random_state=42)\nmodel.fit(X_train, y_train)\n\n# Step 2: Evaluate the model on validation set\ny_pred = model.predict(X_val)\naccuracy = accuracy_score(y_val, y_pred)\n\nprint(\"Model Accuracy on Validation Set:\", accuracy)\nprint(\"\\nClassification Report:\\n\", classification_report(y_val, y_pred))\nprint(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_val, y_pred))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-28T11:55:33.298380Z","iopub.execute_input":"2024-12-28T11:55:33.298725Z","iopub.status.idle":"2024-12-28T11:55:33.524206Z","shell.execute_reply.started":"2024-12-28T11:55:33.298699Z","shell.execute_reply":"2024-12-28T11:55:33.522881Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 1: Import RandomForestClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Step 2: Train Random Forest model with class weights\nrf_model = RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced')\nrf_model.fit(X_train, y_train)\n\n# Step 3: Evaluate the Random Forest model\ny_pred_rf = rf_model.predict(X_val)\nrf_accuracy = accuracy_score(y_val, y_pred_rf)\n\nprint(\"Random Forest Model Accuracy on Validation Set:\", rf_accuracy)\nprint(\"\\nClassification Report:\\n\", classification_report(y_val, y_pred_rf))\nprint(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_val, y_pred_rf))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-28T11:56:38.935169Z","iopub.execute_input":"2024-12-28T11:56:38.935563Z","iopub.status.idle":"2024-12-28T11:56:39.529802Z","shell.execute_reply.started":"2024-12-28T11:56:38.935532Z","shell.execute_reply":"2024-12-28T11:56:39.528695Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\n\n# Define parameter grid for Random Forest\nparam_grid = {\n    'n_estimators': [100, 200, 300],\n    'max_depth': [None, 10, 20, 30],\n    'min_samples_split': [2, 5, 10],\n    'max_features': ['auto', 'sqrt', 'log2']\n}\n\n# Initialize Random Forest with class weights\nrf_model = RandomForestClassifier(random_state=42, class_weight='balanced')\n\n# Perform Grid Search with cross-validation\ngrid_search = GridSearchCV(estimator=rf_model, param_grid=param_grid, \n                           cv=3, scoring='accuracy', verbose=2, n_jobs=-1)\ngrid_search.fit(X_train, y_train)\n\n# Best parameters and model performance\nbest_params = grid_search.best_params_\nbest_model = grid_search.best_estimator_\nbest_score = grid_search.best_score_\n\nprint(\"Best Parameters:\", best_params)\nprint(\"Best Cross-Validation Accuracy:\", best_score)\n\n# Evaluate on validation set\ny_pred_tuned = best_model.predict(X_val)\naccuracy_tuned = accuracy_score(y_val, y_pred_tuned)\n\nprint(\"Tuned Model Accuracy on Validation Set:\", accuracy_tuned)\nprint(\"\\nClassification Report:\\n\", classification_report(y_val, y_pred_tuned))\nprint(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_val, y_pred_tuned))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-28T11:57:59.900593Z","iopub.execute_input":"2024-12-28T11:57:59.901003Z","iopub.status.idle":"2024-12-28T11:58:56.806830Z","shell.execute_reply.started":"2024-12-28T11:57:59.900970Z","shell.execute_reply":"2024-12-28T11:58:56.805343Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 1: Prepare the test dataset for prediction\nX_test = test_df.drop(columns=['PassengerId', 'Ticket'])  # Drop unnecessary columns\n\n# Step 2: Predict survival on the test dataset using the tuned model\ntest_df['Survived'] = best_model.predict(X_test)\n\n# Step 3: Prepare submission file\nsubmission = test_df[['PassengerId', 'Survived']]\nsubmission.to_csv('submission.csv', index=False)\n\nprint(\"Submission file 'submission.csv' created successfully!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-28T12:00:13.645103Z","iopub.execute_input":"2024-12-28T12:00:13.645574Z","iopub.status.idle":"2024-12-28T12:00:13.704588Z","shell.execute_reply.started":"2024-12-28T12:00:13.645541Z","shell.execute_reply":"2024-12-28T12:00:13.703096Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}